{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cosmetic-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 18 04:56:17 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 38%   40C    P8    19W / 250W |   9292MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    215345      C   ...nvs/rfa-stf/bin/python3.8     9289MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "progressive-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM, \n",
    "    AutoTokenizer, \n",
    "    TextDataset, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    Trainer, \n",
    "    PretrainedConfig, \n",
    "    AutoConfig, \n",
    "    MODEL_FOR_MASKED_LM_MAPPING\n",
    ")\n",
    "from fairseq.random_feature_attention.causal_attention import CausalAttention\n",
    "from fairseq.random_feature_attention.utils import load_random_matrices, sample_random_matrices\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "precise-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFASelfAttention(CausalAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False\n",
    "    ):\n",
    "        random_matrix = sample_random_matrices(\n",
    "            num_layers=1,\n",
    "            num_heads=self.num_heads,\n",
    "            random_matrices=self.random_matrices,\n",
    "            is_training=True)[0] if self.training else self.random_matrix_eval\n",
    "        \n",
    "        return (super().forward(hidden_states, random_matrix),)\n",
    "    \n",
    "class AutoModelRFAForMaskedLM(AutoModelForMaskedLM):\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, rfa_args=None, *model_args, **kwargs):\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            config, kwargs = AutoConfig.from_pretrained(\n",
    "                pretrained_model_name_or_path, return_unused_kwargs=True, **kwargs\n",
    "            )\n",
    "\n",
    "        if type(config) in MODEL_FOR_MASKED_LM_MAPPING.keys():\n",
    "            model = MODEL_FOR_MASKED_LM_MAPPING[type(config)].from_pretrained(\n",
    "                pretrained_model_name_or_path, *model_args, config=config, **kwargs\n",
    "            )\n",
    "            \n",
    "            rfa_args.embed_dim = config.hidden_size\n",
    "            rfa_args.num_heads = config.num_attention_heads\n",
    "            rfa_args.head_dim = int(config.hidden_size / config.num_attention_heads)\n",
    "            rfa_args.attn_act = config.hidden_act\n",
    "\n",
    "            model.random_matrices = load_random_matrices(\n",
    "                head_dim=rfa_args.head_dim,\n",
    "                proj_dim=rfa_args.causal_proj_dim,\n",
    "                dtype=torch.float32)\n",
    "            \n",
    "            random_matrices_eval = sample_random_matrices(\n",
    "                num_layers=config.num_hidden_layers,\n",
    "                num_heads=rfa_args.num_heads,\n",
    "                random_matrices=model.random_matrices,\n",
    "                is_training=False)\n",
    "            model.random_matrices_eval = nn.Parameter(random_matrices_eval)\n",
    "            \n",
    "            for idx, layer in enumerate(model._modules[config.model_type].encoder.layer):\n",
    "                layer.attention.self = RFASelfAttention(args=rfa_args, embed_dim=rfa_args.embed_dim, num_heads=rfa_args.num_heads, \n",
    "                                                               head_dim=rfa_args.head_dim)\n",
    "                layer.attention.self.random_matrices = model.random_matrices\n",
    "                layer.attention.self.random_matrix_eval = model.random_matrices_eval\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        raise ValueError(\n",
    "            \"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\n",
    "            \"Model type should be one of {}.\".format(\n",
    "                config.__class__, cls.__name__, \", \".join(c.__name__ for c in MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fantastic-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "def pretrain_and_evaluate_load_data(args, model, tokenizer, dataset, eval_only=True, model_path=\".\"):\n",
    "    datasets = load_dataset(*dataset)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=4,\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=lm_datasets[\"train\"],\n",
    "        eval_dataset=lm_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "        \n",
    "    eval_results = trainer.evaluate()\n",
    "    logger.info(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "    \n",
    "    if not eval_only:\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_results = trainer.evaluate()\n",
    "        logger.info(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                              file_path=args.val_datapath,\n",
    "                              block_size=512)\n",
    "    if eval_only:\n",
    "        train_dataset = val_dataset\n",
    "    else:\n",
    "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
    "        train_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                                    file_path=args.train_datapath,\n",
    "                                    block_size=512)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
    "                      train_dataset=train_dataset, eval_dataset=val_dataset)\n",
    "\n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "    \n",
    "    if not eval_only:\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "handled-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RFAArguments:        \n",
    "    random_feature: str = field(default=\"rrf\")\n",
    "    init_scale: float = field(default=1.0)\n",
    "    causal_proj_dim: int = field(default=64, metadata={\"help\": \"projection size for rfa\"})\n",
    "    learned_tau: bool = field(default=False)\n",
    "    norm_rescale: bool = field(default=False)\n",
    "    cuda_causal_rfa: bool = field(default=False)\n",
    "    use_input_gate: bool = field(default=False)\n",
    "\n",
    "parser = HfArgumentParser((TrainingArguments, RFAArguments,))\n",
    "\n",
    "training_args, rfa_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
    "    '--output_dir', 'tmp',\n",
    "    '--warmup_steps', '500',\n",
    "    '--learning_rate', '0.00003',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--adam_epsilon', '1e-6',\n",
    "    '--max_steps', '3000',\n",
    "    '--logging_steps', '500',\n",
    "    '--save_steps', '500',\n",
    "    '--max_grad_norm', '5.0',\n",
    "    '--per_gpu_eval_batch_size', '8',\n",
    "    '--per_gpu_train_batch_size', '2',  \n",
    "    '--gradient_accumulation_steps', '32',\n",
    "    '--evaluation_strategy', 'epoch',\n",
    "    '--prediction_loss_only', 'True',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "])\n",
    "# training_args.val_datapath = 'wikitext-103-raw/wiki.valid.raw'\n",
    "# training_args.train_datapath = 'wikitext-103-raw/wiki.train.raw'\n",
    "\n",
    "# Choose GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "frozen-dream",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelRFAForMaskedLM.from_pretrained('roberta-base', rfa_args=rfa_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "noted-revelation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset wikitext (/usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-582408dcf89f813a.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-5f0458d2af149e47.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-7e282fad9847b7d2.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-1fc46ccb88f495f6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-2497a07e7b9a5543.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-c5c1bbcd49587770.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-5e5b833c388e1260.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-d79db16a548ce91b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-8e99e9fbff69b389.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-e1832d177b311be9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-7995798310fdc597.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-5db3ac2c4f7ba5d9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-cf88371f70c09d31.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-df2242053e81d673.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-53e57fbe6c25e63c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-b9f9c6e30319c13b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-436beba3d0c60f39.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-e99f962601ccef62.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-0d0ae502e4405ce0.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-624c839c19670625.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-282312586cd0b722.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-42e32874455e4e08.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-191f324b226a5a0c.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /usr/lusers/dzhu99/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91/cache-f09a0bc37e838b91.arrow\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Perplexity: 3200635.37\n",
      "Some weights of the model checkpoint at tmp were not used when initializing RobertaForMaskedLM: ['random_matrices', 'random_matrices_eval', 'roberta.encoder.layer.0.attention.self.tau', 'roberta.encoder.layer.0.attention.self.random_matrices', 'roberta.encoder.layer.0.attention.self.random_matrix_eval', 'roberta.encoder.layer.0.attention.self.q_proj.weight', 'roberta.encoder.layer.0.attention.self.q_proj.bias', 'roberta.encoder.layer.0.attention.self.k_proj.weight', 'roberta.encoder.layer.0.attention.self.k_proj.bias', 'roberta.encoder.layer.0.attention.self.v_proj.weight', 'roberta.encoder.layer.0.attention.self.v_proj.bias', 'roberta.encoder.layer.0.attention.self.out_proj.weight', 'roberta.encoder.layer.0.attention.self.out_proj.bias', 'roberta.encoder.layer.1.attention.self.tau', 'roberta.encoder.layer.1.attention.self.random_matrices', 'roberta.encoder.layer.1.attention.self.random_matrix_eval', 'roberta.encoder.layer.1.attention.self.q_proj.weight', 'roberta.encoder.layer.1.attention.self.q_proj.bias', 'roberta.encoder.layer.1.attention.self.k_proj.weight', 'roberta.encoder.layer.1.attention.self.k_proj.bias', 'roberta.encoder.layer.1.attention.self.v_proj.weight', 'roberta.encoder.layer.1.attention.self.v_proj.bias', 'roberta.encoder.layer.1.attention.self.out_proj.weight', 'roberta.encoder.layer.1.attention.self.out_proj.bias', 'roberta.encoder.layer.2.attention.self.tau', 'roberta.encoder.layer.2.attention.self.random_matrices', 'roberta.encoder.layer.2.attention.self.random_matrix_eval', 'roberta.encoder.layer.2.attention.self.q_proj.weight', 'roberta.encoder.layer.2.attention.self.q_proj.bias', 'roberta.encoder.layer.2.attention.self.k_proj.weight', 'roberta.encoder.layer.2.attention.self.k_proj.bias', 'roberta.encoder.layer.2.attention.self.v_proj.weight', 'roberta.encoder.layer.2.attention.self.v_proj.bias', 'roberta.encoder.layer.2.attention.self.out_proj.weight', 'roberta.encoder.layer.2.attention.self.out_proj.bias', 'roberta.encoder.layer.3.attention.self.tau', 'roberta.encoder.layer.3.attention.self.random_matrices', 'roberta.encoder.layer.3.attention.self.random_matrix_eval', 'roberta.encoder.layer.3.attention.self.q_proj.weight', 'roberta.encoder.layer.3.attention.self.q_proj.bias', 'roberta.encoder.layer.3.attention.self.k_proj.weight', 'roberta.encoder.layer.3.attention.self.k_proj.bias', 'roberta.encoder.layer.3.attention.self.v_proj.weight', 'roberta.encoder.layer.3.attention.self.v_proj.bias', 'roberta.encoder.layer.3.attention.self.out_proj.weight', 'roberta.encoder.layer.3.attention.self.out_proj.bias', 'roberta.encoder.layer.4.attention.self.tau', 'roberta.encoder.layer.4.attention.self.random_matrices', 'roberta.encoder.layer.4.attention.self.random_matrix_eval', 'roberta.encoder.layer.4.attention.self.q_proj.weight', 'roberta.encoder.layer.4.attention.self.q_proj.bias', 'roberta.encoder.layer.4.attention.self.k_proj.weight', 'roberta.encoder.layer.4.attention.self.k_proj.bias', 'roberta.encoder.layer.4.attention.self.v_proj.weight', 'roberta.encoder.layer.4.attention.self.v_proj.bias', 'roberta.encoder.layer.4.attention.self.out_proj.weight', 'roberta.encoder.layer.4.attention.self.out_proj.bias', 'roberta.encoder.layer.5.attention.self.tau', 'roberta.encoder.layer.5.attention.self.random_matrices', 'roberta.encoder.layer.5.attention.self.random_matrix_eval', 'roberta.encoder.layer.5.attention.self.q_proj.weight', 'roberta.encoder.layer.5.attention.self.q_proj.bias', 'roberta.encoder.layer.5.attention.self.k_proj.weight', 'roberta.encoder.layer.5.attention.self.k_proj.bias', 'roberta.encoder.layer.5.attention.self.v_proj.weight', 'roberta.encoder.layer.5.attention.self.v_proj.bias', 'roberta.encoder.layer.5.attention.self.out_proj.weight', 'roberta.encoder.layer.5.attention.self.out_proj.bias', 'roberta.encoder.layer.6.attention.self.tau', 'roberta.encoder.layer.6.attention.self.random_matrices', 'roberta.encoder.layer.6.attention.self.random_matrix_eval', 'roberta.encoder.layer.6.attention.self.q_proj.weight', 'roberta.encoder.layer.6.attention.self.q_proj.bias', 'roberta.encoder.layer.6.attention.self.k_proj.weight', 'roberta.encoder.layer.6.attention.self.k_proj.bias', 'roberta.encoder.layer.6.attention.self.v_proj.weight', 'roberta.encoder.layer.6.attention.self.v_proj.bias', 'roberta.encoder.layer.6.attention.self.out_proj.weight', 'roberta.encoder.layer.6.attention.self.out_proj.bias', 'roberta.encoder.layer.7.attention.self.tau', 'roberta.encoder.layer.7.attention.self.random_matrices', 'roberta.encoder.layer.7.attention.self.random_matrix_eval', 'roberta.encoder.layer.7.attention.self.q_proj.weight', 'roberta.encoder.layer.7.attention.self.q_proj.bias', 'roberta.encoder.layer.7.attention.self.k_proj.weight', 'roberta.encoder.layer.7.attention.self.k_proj.bias', 'roberta.encoder.layer.7.attention.self.v_proj.weight', 'roberta.encoder.layer.7.attention.self.v_proj.bias', 'roberta.encoder.layer.7.attention.self.out_proj.weight', 'roberta.encoder.layer.7.attention.self.out_proj.bias', 'roberta.encoder.layer.8.attention.self.tau', 'roberta.encoder.layer.8.attention.self.random_matrices', 'roberta.encoder.layer.8.attention.self.random_matrix_eval', 'roberta.encoder.layer.8.attention.self.q_proj.weight', 'roberta.encoder.layer.8.attention.self.q_proj.bias', 'roberta.encoder.layer.8.attention.self.k_proj.weight', 'roberta.encoder.layer.8.attention.self.k_proj.bias', 'roberta.encoder.layer.8.attention.self.v_proj.weight', 'roberta.encoder.layer.8.attention.self.v_proj.bias', 'roberta.encoder.layer.8.attention.self.out_proj.weight', 'roberta.encoder.layer.8.attention.self.out_proj.bias', 'roberta.encoder.layer.9.attention.self.tau', 'roberta.encoder.layer.9.attention.self.random_matrices', 'roberta.encoder.layer.9.attention.self.random_matrix_eval', 'roberta.encoder.layer.9.attention.self.q_proj.weight', 'roberta.encoder.layer.9.attention.self.q_proj.bias', 'roberta.encoder.layer.9.attention.self.k_proj.weight', 'roberta.encoder.layer.9.attention.self.k_proj.bias', 'roberta.encoder.layer.9.attention.self.v_proj.weight', 'roberta.encoder.layer.9.attention.self.v_proj.bias', 'roberta.encoder.layer.9.attention.self.out_proj.weight', 'roberta.encoder.layer.9.attention.self.out_proj.bias', 'roberta.encoder.layer.10.attention.self.tau', 'roberta.encoder.layer.10.attention.self.random_matrices', 'roberta.encoder.layer.10.attention.self.random_matrix_eval', 'roberta.encoder.layer.10.attention.self.q_proj.weight', 'roberta.encoder.layer.10.attention.self.q_proj.bias', 'roberta.encoder.layer.10.attention.self.k_proj.weight', 'roberta.encoder.layer.10.attention.self.k_proj.bias', 'roberta.encoder.layer.10.attention.self.v_proj.weight', 'roberta.encoder.layer.10.attention.self.v_proj.bias', 'roberta.encoder.layer.10.attention.self.out_proj.weight', 'roberta.encoder.layer.10.attention.self.out_proj.bias', 'roberta.encoder.layer.11.attention.self.tau', 'roberta.encoder.layer.11.attention.self.random_matrices', 'roberta.encoder.layer.11.attention.self.random_matrix_eval', 'roberta.encoder.layer.11.attention.self.q_proj.weight', 'roberta.encoder.layer.11.attention.self.q_proj.bias', 'roberta.encoder.layer.11.attention.self.k_proj.weight', 'roberta.encoder.layer.11.attention.self.k_proj.bias', 'roberta.encoder.layer.11.attention.self.v_proj.weight', 'roberta.encoder.layer.11.attention.self.v_proj.bias', 'roberta.encoder.layer.11.attention.self.out_proj.weight', 'roberta.encoder.layer.11.attention.self.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at tmp and are newly initialized: ['roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/3000 00:11 < 3:11:37, 0.26 it/s, Epoch 0.05/41]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-3b0dd90ca61a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrain_and_evaluate_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'wikitext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wikitext-2-raw-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-176-41729c916e54>\u001b[0m in \u001b[0;36mpretrain_and_evaluate_load_data\u001b[0;34m(args, model, tokenizer, dataset, eval_only, model_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meval_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/stf/dzhu99/envs/rfa-stf/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/stf/dzhu99/envs/rfa-stf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/stf/dzhu99/envs/rfa-stf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/stf/dzhu99/envs/rfa-stf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/stf/dzhu99/envs/rfa-stf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/stf/dzhu99/envs/rfa-stf/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0mCan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \"\"\"\n\u001b[0;32m-> 1125\u001b[0;31m         return self._getitem(\n\u001b[0m\u001b[1;32m   1126\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0mformat_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain_and_evaluate_load_data(training_args, model, tokenizer, ('wikitext', 'wikitext-2-raw-v1'), eval_only=False, model_path=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-bunch",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast\n",
    "\n",
    "roberta_base = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "logger.info('Evaluating roberta-base (seqlen: 512) for reference ...')\n",
    "pretrain_and_evaluate_load_data(training_args, roberta_base, roberta_base_tokenizer, ('wikitext', 'wikitext-2-raw-v1'), model_path=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_and_evaluate_load_data(training_args, roberta_base, roberta_base_tokenizer, ('wikitext', 'wikitext-2-raw-v1'), model_path=training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
