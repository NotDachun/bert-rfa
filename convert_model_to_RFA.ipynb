{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "convert_model_to_RFA.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NotDachun/huggingface-ark/blob/main/convert_model_to_RFA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtyhTQEyC6eC",
        "outputId": "b9362ad5-7447-4f82-c781-e928b1d5b56d"
      },
      "source": [
        "# Uncomment to run this notebook in Google Colab\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/haopeng-uw/RFA.git'.format(user, password)\n",
        "\n",
        "!$cmd_string\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "id": "NtyhTQEyC6eC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: notdachun\n",
            "Password: ··········\n",
            "fatal: destination path 'RFA' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuZi0sqiDm_m",
        "outputId": "6b966400-f378-4f37-94d9-4d765ea0eefd"
      },
      "source": [
        "# Uncomment to run this notebook in Google Colab\n",
        "# Need to manually remove dataclasses from setup.py, otherwise errors out\n",
        "%cd /content/RFA\n",
        "!pip install --editable ./\n",
        "\n",
        "%cd /content/RFA/fairseq/random_feature_attention\n",
        "!python setup.py install\n",
        "\n",
        "%cd /content/RFA/fairseq/linfa\n",
        "!python setup.py install"
      ],
      "id": "LuZi0sqiDm_m",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/RFA\n",
            "Obtaining file:///content/RFA\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (1.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (4.41.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (0.5.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (2019.12.20)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (1.1.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (0.29.23)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (1.9.0+cu102)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+740a608) (1.19.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+740a608) (2.20)\n",
            "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==1.0.0a0+740a608) (2.1.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==1.0.0a0+740a608) (5.1.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq==1.0.0a0+740a608) (4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq==1.0.0a0+740a608) (3.7.4.3)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+740a608) (2.0.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq==1.0.0a0+740a608) (5.4.1)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq==1.0.0a0+740a608) (3.4.1)\n",
            "Installing collected packages: fairseq\n",
            "  Found existing installation: fairseq 1.0.0a0+740a608\n",
            "    Can't uninstall 'fairseq'. No files were found to uninstall.\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed fairseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97NpoljcAhsl",
        "outputId": "1c3e2986-f0df-4a17-f86f-0a64a88bcdf5"
      },
      "source": [
        "# Uncomment to run this notebook in Google Colab\n",
        "!pip install transformers datasets"
      ],
      "id": "97NpoljcAhsl",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.6.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "detected-botswana",
        "outputId": "c2bf57db-1901-4b92-c54e-5d266a1a3a69"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "detected-botswana",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 20 16:03:23 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "official-order"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from argparse import Namespace\n",
        "from dataclasses import dataclass, field\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM, \n",
        "    AutoTokenizer, \n",
        "    RobertaForMaskedLM,\n",
        "    TextDataset, \n",
        "    DataCollatorForLanguageModeling, \n",
        "    Trainer, \n",
        "    PretrainedConfig, \n",
        "    AutoConfig, \n",
        "    MODEL_FOR_MASKED_LM_MAPPING\n",
        ")\n",
        "from fairseq.random_feature_attention.causal_attention import CausalAttention\n",
        "from fairseq.random_feature_attention.utils import load_random_matrices, sample_random_matrices\n",
        "from transformers import TrainingArguments, HfArgumentParser\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "id": "official-order",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "senior-performance"
      },
      "source": [
        "class RFASelfAttention(CausalAttention):\n",
        "    def __init__(self, args, embed_dim, num_heads, head_dim):\n",
        "        super().__init__(args=args, embed_dim=embed_dim, num_heads=num_heads, head_dim=head_dim)\n",
        "        self.random_matrices = None\n",
        "        self.random_matrix_eval = None\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False\n",
        "    ):\n",
        "        random_matrix = sample_random_matrices(\n",
        "            num_layers=1,\n",
        "            num_heads=self.num_heads,\n",
        "            random_matrices=self.random_matrices,\n",
        "            is_training=True)[0] if self.training else self.random_matrix_eval\n",
        "        \n",
        "        return (super().forward(hidden_states, random_matrix),)\n",
        "    \n",
        "# class RobertaRFAForMaskedLM(RobertaForMaskedLM):\n",
        "#     def __init__(self, config, rfa_args=None):\n",
        "#         super().__init__(config)\n",
        "#         rfa_args.embed_dim = config.hidden_size\n",
        "#         rfa_args.num_heads = config.num_attention_heads\n",
        "#         rfa_args.head_dim = int(config.hidden_size / config.num_attention_heads)\n",
        "#         rfa_args.attn_act = config.hidden_act\n",
        "        \n",
        "#         for layer in self.roberta.encoder.layer:                \n",
        "#             layer.attention.self = RFASelfAttention(args=rfa_args, embed_dim=rfa_args.embed_dim, num_heads=rfa_args.num_heads, \n",
        "#                                                                head_dim=rfa_args.head_dim)\n",
        "    \n",
        "class AutoModelRFAForMaskedLM(AutoModelForMaskedLM):\n",
        "    \n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, rfa_args=None, *model_args, **kwargs):\n",
        "        config = kwargs.pop(\"config\", None)\n",
        "        if not isinstance(config, PretrainedConfig):\n",
        "            config, kwargs = AutoConfig.from_pretrained(\n",
        "                pretrained_model_name_or_path, return_unused_kwargs=True, **kwargs\n",
        "            )\n",
        "        \n",
        "            model = MODEL_FOR_MASKED_LM_MAPPING[type(config)]\n",
        "    \n",
        "            rfa_args.embed_dim = config.hidden_size\n",
        "            rfa_args.num_heads = config.num_attention_heads\n",
        "            rfa_args.head_dim = int(config.hidden_size / config.num_attention_heads)\n",
        "            rfa_args.attn_act = config.hidden_act\n",
        "\n",
        "            model.random_matrices = load_random_matrices(\n",
        "                head_dim=rfa_args.head_dim,\n",
        "                proj_dim=rfa_args.causal_proj_dim,\n",
        "                dtype=torch.float32)\n",
        "            \n",
        "            random_matrices_eval = sample_random_matrices(\n",
        "                num_layers=config.num_hidden_layers,\n",
        "                num_heads=rfa_args.num_heads,\n",
        "                random_matrices=model.random_matrices,\n",
        "                is_training=False)\n",
        "            model.random_matrices_eval = nn.Parameter(random_matrices_eval)\n",
        "            \n",
        "            for idx, layer in enumerate(model._modules[config.model_type].encoder.layer):\n",
        "                rfa_self_attn = RFASelfAttention(args=rfa_args, embed_dim=rfa_args.embed_dim, num_heads=rfa_args.num_heads, \n",
        "                                                               head_dim=rfa_args.head_dim)\n",
        "                rfa_self_attn.random_matrices = model.random_matrices\n",
        "                rfa_self_attn.random_matrix_eval = model.random_matrices_eval\n",
        "                \n",
        "                layer.attention.self = rfa_self_attn\n",
        "            \n",
        "            if type(config) in MODEL_FOR_MASKED_LM_MAPPING.keys():\n",
        "                model.from_pretrained(\n",
        "                    pretrained_model_name_or_path, *model_args, config=config, **kwargs\n",
        "                )\n",
        "            return model\n",
        "            \n",
        "        raise ValueError(\n",
        "            \"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\n",
        "            \"Model type should be one of {}.\".format(\n",
        "                config.__class__, cls.__name__, \", \".join(c.__name__ for c in MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
        "            )\n",
        "        )"
      ],
      "id": "senior-performance",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "magnetic-volleyball"
      },
      "source": [
        "block_size = 512\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "    \n",
        "def pretrain_and_evaluate_load_data(args, model, tokenizer, dataset, eval_only=True, model_path=\".\"):\n",
        "    datasets = load_dataset(*dataset)\n",
        "    tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "    lm_datasets = tokenized_datasets.map(\n",
        "        group_texts,\n",
        "        batched=True,\n",
        "        batch_size=1000,\n",
        "        num_proc=4,\n",
        "    )\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=lm_datasets[\"train\"],\n",
        "        eval_dataset=lm_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "        \n",
        "    eval_results = trainer.evaluate()\n",
        "    logger.info(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
        "    \n",
        "    if not eval_only:\n",
        "        trainer.train(model_path=model_path)\n",
        "        trainer.save_model()\n",
        "\n",
        "        eval_results = trainer.evaluate()\n",
        "        logger.info(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
        "\n",
        "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
        "    val_dataset = TextDataset(tokenizer=tokenizer,\n",
        "                              file_path=args.val_datapath,\n",
        "                              block_size=512)\n",
        "    if eval_only:\n",
        "        train_dataset = val_dataset\n",
        "    else:\n",
        "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
        "        train_dataset = TextDataset(tokenizer=tokenizer,\n",
        "                                    file_path=args.train_datapath,\n",
        "                                    block_size=512)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
        "                      train_dataset=train_dataset, eval_dataset=val_dataset)\n",
        "\n",
        "    eval_loss = trainer.evaluate()\n",
        "    eval_loss = eval_loss['eval_loss']\n",
        "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
        "    \n",
        "    if not eval_only:\n",
        "        trainer.train(model_path=model_path)\n",
        "        trainer.save_model()\n",
        "\n",
        "        eval_loss = trainer.evaluate()\n",
        "        eval_loss = eval_loss['eval_loss']\n",
        "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')"
      ],
      "id": "magnetic-volleyball",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "contained-pitch"
      },
      "source": [
        "@dataclass\n",
        "class RFAArguments:        \n",
        "    random_feature: str = field(default=\"rrf\")\n",
        "    init_scale: float = field(default=1.0)\n",
        "    # experiment with different dim\n",
        "    causal_proj_dim: int = field(default=64, metadata={\"help\": \"projection size for rfa\"}) \n",
        "    learned_tau: bool = field(default=False)\n",
        "        \n",
        "    # experiment with true\n",
        "    norm_rescale: bool = field(default=False)\n",
        "    cuda_causal_rfa: bool = field(default=False)\n",
        "        \n",
        "    # experiment with true\n",
        "    use_input_gate: bool = field(default=False)\n",
        "\n",
        "parser = HfArgumentParser((TrainingArguments, RFAArguments,))\n",
        "\n",
        "training_args, rfa_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
        "    '--output_dir', 'tmp',\n",
        "    '--warmup_steps', '500',\n",
        "    '--learning_rate', '0.00003',\n",
        "    '--weight_decay', '0.01',\n",
        "    '--adam_epsilon', '1e-6',\n",
        "    '--max_steps', '3000',\n",
        "    '--logging_steps', '500',\n",
        "    '--save_steps', '500',\n",
        "    '--max_grad_norm', '5.0',\n",
        "    '--per_gpu_eval_batch_size', '8',\n",
        "    '--per_gpu_train_batch_size', '2',  \n",
        "    '--gradient_accumulation_steps', '32',\n",
        "    '--evaluation_strategy', 'epoch',\n",
        "    '--prediction_loss_only', 'True',\n",
        "    '--do_train',\n",
        "    '--do_eval',\n",
        "])\n",
        "# training_args.val_datapath = 'wikitext-103-raw/wiki.valid.raw'\n",
        "# training_args.train_datapath = 'wikitext-103-raw/wiki.train.raw'\n",
        "\n",
        "# Choose GPU\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "id": "contained-pitch",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "varying-ukraine",
        "outputId": "c2ddab60-19d8-41e3-b6a6-66294f870e5d"
      },
      "source": [
        "model = AutoModelRFAForMaskedLM.from_pretrained('roberta-base', rfa_args=rfa_args, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True, local_files_only=True)\n",
        "config = model.config"
      ],
      "id": "varying-ukraine",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cannot find the requested files in the cached path and outgoing traffic has been disabled. To enable model look-ups and downloads online, set 'local_files_only' to False.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1336\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1546\u001b[0m                     raise FileNotFoundError(\n\u001b[0;32m-> 1547\u001b[0;31m                         \u001b[0;34m\"Cannot find the requested files in the cached path and outgoing traffic has been\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1548\u001b[0m                         \u001b[0;34m\" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find the requested files in the cached path and outgoing traffic has been disabled. To enable model look-ups and downloads online, set 'local_files_only' to False.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-61cfe9d7a29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelRFAForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfa_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrfa_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3373c8fb844a>\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, rfa_args, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \"\"\"\n\u001b[1;32m    445\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             )\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load config for 'roberta-base'. Make sure that:\n\n- 'roberta-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'roberta-base' is the correct path to a directory containing a config.json file\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "affected-outreach"
      },
      "source": [
        "model.state_dict()"
      ],
      "id": "affected-outreach",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sitting-cargo"
      },
      "source": [
        "model.save_pretraine"
      ],
      "id": "sitting-cargo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sensitive-vertical"
      },
      "source": [
        "ls"
      ],
      "id": "sensitive-vertical",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impossible-brave"
      },
      "source": [
        "model.save_pretrained(\"roberta-test\")"
      ],
      "id": "impossible-brave",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "committed-disease"
      },
      "source": [
        ""
      ],
      "id": "committed-disease",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "streaming-extent"
      },
      "source": [
        "model.__dict__"
      ],
      "id": "streaming-extent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "organized-ferry"
      },
      "source": [
        "AutoModelRFAForMaskedLM.from_pretrained(\"roberta-test\")"
      ],
      "id": "organized-ferry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "found-sixth"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "model = RobertaRFAForMaskedLM.from_pretrained(\"roberta-test\", rfa_args=rfa_args)"
      ],
      "id": "found-sixth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "biological-eligibility"
      },
      "source": [
        "model.state_dict()"
      ],
      "id": "biological-eligibility",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "accessory-backup"
      },
      "source": [
        "model.__dict__"
      ],
      "id": "accessory-backup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "liquid-indicator"
      },
      "source": [
        "model.from_pretrained(\"roberta-test\", rfa_args=rfa_args)"
      ],
      "id": "liquid-indicator",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "constant-wings"
      },
      "source": [
        "model.load_state_dict(torch.load(\"roberta-test/pytorch_model.bin\"))"
      ],
      "id": "constant-wings",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "french-anthropology"
      },
      "source": [
        "pretrain_and_evaluate_load_data(training_args, model, tokenizer, ('wikitext', 'wikitext-2-raw-v1'), eval_only=False, model_path=\"roberta-rfa\")"
      ],
      "id": "french-anthropology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "racial-treasurer"
      },
      "source": [
        "from transformers import RobertaForMaskedLM, RobertaTokenizerFast\n",
        "\n",
        "roberta_base = RobertaForMaskedLM.from_pretrained('roberta-base', local_files_only=True)\n",
        "roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', local_files_only=True)\n",
        "logger.info('Evaluating roberta-base (seqlen: 512) for reference ...')\n",
        "pretrain_and_evaluate_load_data(training_args, roberta_base, roberta_base_tokenizer, ('wikitext', 'wikitext-2-raw-v1'), model_path=training_args.output_dir)"
      ],
      "id": "racial-treasurer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "macro-investigator"
      },
      "source": [
        "pretrain_and_evaluate_load_data(training_args, roberta_base, roberta_base_tokenizer, ('wikitext', 'wikitext-2-raw-v1'), eval_only=False, model_path=\"roberta-baseline\")"
      ],
      "id": "macro-investigator",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "capable-pixel"
      },
      "source": [
        "1. Get the roberta data\n",
        "2. Figure out how to fix the configuration to use RFA"
      ],
      "id": "capable-pixel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buried-diagnosis"
      },
      "source": [
        ""
      ],
      "id": "buried-diagnosis",
      "execution_count": null,
      "outputs": []
    }
  ]
}